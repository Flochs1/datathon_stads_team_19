{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:39.721833Z",
     "start_time": "2025-03-07T16:36:39.717189Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:39.794533Z",
     "start_time": "2025-03-07T16:36:39.782706Z"
    }
   },
   "source": [
    "# importing utilities\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# importing data science libraries\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "# importing pytorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "from torch.utils.data import DataLoader\n",
    "from autoencoder import encoder, decoder\n",
    "\n",
    "# import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from IPython.display import Image, display\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# ignore potential warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:39.811272Z",
     "start_time": "2025-03-07T16:36:39.798475Z"
    }
   },
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if CDNN is available on the server let's execute the cell below to display information about the available CUDNN version:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:39.864155Z",
     "start_time": "2025-03-07T16:36:39.858836Z"
    }
   },
   "source": [
    "# print CUDNN backend version\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] The CUDNN backend version: {}'.format(now, torch.backends.cudnn.version()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20250307-16:36:39] The CUDNN backend version: 90501\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDNN and GPU's are available let's still specify if we want to use both:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:39.909507Z",
     "start_time": "2025-03-07T16:36:39.905418Z"
    }
   },
   "source": "USE_CUDA = True",
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:40.006717Z",
     "start_time": "2025-03-07T16:36:39.982616Z"
    }
   },
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234 #1234\n",
    "rd.seed(seed_value) # set random seed\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:40.374965Z",
     "start_time": "2025-03-07T16:36:40.006717Z"
    }
   },
   "source": [
    "# load the dataset into the notebook kernel\n",
    "ori_dataset = pd.read_csv('./data/datathon_data.csv')"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:40.489671Z",
     "start_time": "2025-03-07T16:36:40.428302Z"
    }
   },
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the lab\n",
    "label = ori_dataset.pop('label')"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:40.793903Z",
     "start_time": "2025-03-07T16:36:40.538501Z"
    }
   },
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:40.872839Z",
     "start_time": "2025-03-07T16:36:40.835477Z"
    }
   },
   "source": [
    "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.087709Z",
     "start_time": "2025-03-07T16:36:40.912996Z"
    }
   },
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.138744Z",
     "start_time": "2025-03-07T16:36:41.127639Z"
    }
   },
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 512, out 256\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 256, out 128\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 128, out 64\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 64, out 32\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 32, out 16\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 7 - in 16, out 8\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 8, out 4\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 4, out 3\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
    "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
    "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
    "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
    "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
    "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
    "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
    "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
    "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
    "        x = self.encoder_R9(self.encoder_L9(x)) # don't apply dropout to the AE bottleneck\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.218778Z",
     "start_time": "2025-03-07T16:36:41.193036Z"
    }
   },
   "source": [
    "# init training network classes / architectures\n",
    "encoder_train = encoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
    "    encoder_train = encoder().cuda()"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.267683Z",
     "start_time": "2025-03-07T16:36:41.258056Z"
    }
   },
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 4\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
    "\n",
    "        # specify layer 2 - in 4, out 8\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 8, out 16\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 32\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 32, out 64\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 64, out 128\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 7 - in 128, out 256\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 256, out 512\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 512, out 618\n",
    "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
    "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # init dropout layer with probability p\n",
    "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
    "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
    "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
    "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
    "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
    "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
    "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
    "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
    "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.336194Z",
     "start_time": "2025-03-07T16:36:41.316718Z"
    }
   },
   "source": [
    "# init training network classes / architectures\n",
    "decoder_train = decoder()\n",
    "\n",
    "# push to cuda if cudnn is available\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    decoder_train = decoder().cuda()\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20250307-16:36:41] decoder architecture:\n",
      "\n",
      "decoder(\n",
      "  (decoder_L1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (decoder_R1): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L2): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (decoder_R2): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L3): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (decoder_R3): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L4): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (decoder_R4): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L5): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (decoder_R5): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L6): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (decoder_R6): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L7): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (decoder_R7): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L8): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (decoder_R8): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (decoder_L9): Linear(in_features=512, out_features=384, bias=True)\n",
      "  (decoder_R9): LeakyReLU(negative_slope=0.4, inplace=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.437134Z",
     "start_time": "2025-03-07T16:36:41.431244Z"
    }
   },
   "source": [
    "# define the optimization criterion / loss function\n",
    "loss_function = nn.BCEWithLogitsLoss(reduction='mean')"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.492204Z",
     "start_time": "2025-03-07T16:36:41.484559Z"
    }
   },
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 1e-3\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:41.552107Z",
     "start_time": "2025-03-07T16:36:41.547307Z"
    }
   },
   "source": [
    "# specify training parameters\n",
    "num_epochs = 10\n",
    "mini_batch_size = 128"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:36:52.436783Z",
     "start_time": "2025-03-07T16:36:41.603507Z"
    }
   },
   "source": [
    "# convert pre-processed data to pytorch tensor\n",
    "array = ori_subset_transformed.values\n",
    "float_array = array.astype(np.float64)\n",
    "torch_dataset = torch.from_numpy(float_array).float()\n",
    "\n",
    "# convert to pytorch tensor - none cuda enabled\n",
    "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
    "# note: we set num_workers to zero to retrieve deterministic results\n",
    "\n",
    "# determine if CUDA is available at compute node\n",
    "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:43:57.101080Z",
     "start_time": "2025-03-07T16:36:52.473856Z"
    }
   },
   "source": [
    "# init collection of mini-batch losses\n",
    "losses = []\n",
    "\n",
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "    \n",
    "    # determine if CUDA is available at compute node\n",
    "    if(torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
    "        \n",
    "        # set networks / models in GPU mode\n",
    "        encoder_train.cuda()\n",
    "        decoder_train.cuda()\n",
    "\n",
    "    # set networks in training mode (apply dropout when needed)\n",
    "    encoder_train.train()\n",
    "    decoder_train.train()\n",
    "\n",
    "    # start timer\n",
    "    start_time = datetime.now()\n",
    "        \n",
    "    # iterate over all mini-batches\n",
    "    for mini_batch_data in dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # convert mini batch to torch variable\n",
    "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
    "\n",
    "        # =================== (1) forward pass ===================================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "        \n",
    "        # =================== (2) compute reconstruction loss ====================\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
    "        \n",
    "        # =================== (3) backward pass ==================================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # =================== (4) update model parameters ========================\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== monitor training progress ==========================\n",
    "\n",
    "        # print training progress each 1'000 mini-batches\n",
    "        if mini_batch_count % 1000 == 0:\n",
    "            \n",
    "            # print the training mode: either on GPU or CPU\n",
    "            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'\n",
    "            \n",
    "            # print mini batch reconstuction results\n",
    "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            end_time = datetime.now() - start_time\n",
    "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))\n",
    "\n",
    "            # reset timer\n",
    "            start_time = datetime.now()\n",
    "\n",
    "    # =================== evaluate model performance =============================\n",
    "    \n",
    "    # set networks in evaluation mode (don't apply dropout)\n",
    "    encoder_train.cpu().eval()\n",
    "    decoder_train.cpu().eval()\n",
    "\n",
    "    # reconstruct encoded transactional data\n",
    "    reconstruction = decoder_train(encoder_train(data))\n",
    "    \n",
    "    # determine reconstruction loss - all transactions\n",
    "    reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "            \n",
    "    # collect reconstruction loss\n",
    "    losses.extend([reconstruction_loss_all.item()])\n",
    "    \n",
    "    # print reconstuction loss results\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss_all.item()))\n",
    "\n",
    "    # =================== save model snapshot to disk ============================\n",
    "    \n",
    "    # save trained encoder model file to disk\n",
    "    encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
    "    torch.save(encoder_train.state_dict(), os.path.join(\"./models\", encoder_model_name))\n",
    "\n",
    "    # save trained decoder model file to disk\n",
    "    decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
    "    torch.save(decoder_train.state_dict(), os.path.join(\"./models\", decoder_model_name))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20250307-16:36:59] training status, epoch: [0001/0010], batch: 1000, loss: 0.0106, mode: GPU, time required: 0:00:07.326386\n",
      "[LOG 20250307-16:37:06] training status, epoch: [0001/0010], batch: 2000, loss: 0.0062, mode: GPU, time required: 0:00:07.034281\n",
      "[LOG 20250307-16:37:14] training status, epoch: [0001/0010], batch: 3000, loss: 0.0055, mode: GPU, time required: 0:00:07.348077\n",
      "[LOG 20250307-16:37:22] training status, epoch: [0001/0010], batch: 4000, loss: 0.0044, mode: GPU, time required: 0:00:07.807643\n",
      "[LOG 20250307-16:37:34] training status, epoch: [0001/0010], loss: 0.0053855889\n",
      "[LOG 20250307-16:37:42] training status, epoch: [0002/0010], batch: 1000, loss: 0.0041, mode: GPU, time required: 0:00:07.678462\n",
      "[LOG 20250307-16:37:49] training status, epoch: [0002/0010], batch: 2000, loss: 0.0037, mode: GPU, time required: 0:00:07.118995\n",
      "[LOG 20250307-16:37:56] training status, epoch: [0002/0010], batch: 3000, loss: 0.0041, mode: GPU, time required: 0:00:07.230089\n",
      "[LOG 20250307-16:38:03] training status, epoch: [0002/0010], batch: 4000, loss: 0.0038, mode: GPU, time required: 0:00:07.233119\n",
      "[LOG 20250307-16:38:15] training status, epoch: [0002/0010], loss: 0.0039599286\n",
      "[LOG 20250307-16:38:23] training status, epoch: [0003/0010], batch: 1000, loss: 0.0036, mode: GPU, time required: 0:00:07.649698\n",
      "[LOG 20250307-16:38:30] training status, epoch: [0003/0010], batch: 2000, loss: 0.005, mode: GPU, time required: 0:00:07.048419\n",
      "[LOG 20250307-16:38:37] training status, epoch: [0003/0010], batch: 3000, loss: 0.0038, mode: GPU, time required: 0:00:07.274180\n",
      "[LOG 20250307-16:38:45] training status, epoch: [0003/0010], batch: 4000, loss: 0.0032, mode: GPU, time required: 0:00:07.385887\n",
      "[LOG 20250307-16:38:57] training status, epoch: [0003/0010], loss: 0.0035773371\n",
      "[LOG 20250307-16:39:05] training status, epoch: [0004/0010], batch: 1000, loss: 0.0035, mode: GPU, time required: 0:00:07.663550\n",
      "[LOG 20250307-16:39:13] training status, epoch: [0004/0010], batch: 2000, loss: 0.0032, mode: GPU, time required: 0:00:07.804218\n",
      "[LOG 20250307-16:39:20] training status, epoch: [0004/0010], batch: 3000, loss: 0.0033, mode: GPU, time required: 0:00:07.091495\n",
      "[LOG 20250307-16:39:27] training status, epoch: [0004/0010], batch: 4000, loss: 0.0033, mode: GPU, time required: 0:00:07.717907\n",
      "[LOG 20250307-16:39:39] training status, epoch: [0004/0010], loss: 0.0031372695\n",
      "[LOG 20250307-16:39:48] training status, epoch: [0005/0010], batch: 1000, loss: 0.0031, mode: GPU, time required: 0:00:08.621399\n",
      "[LOG 20250307-16:39:55] training status, epoch: [0005/0010], batch: 2000, loss: 0.0029, mode: GPU, time required: 0:00:07.527094\n",
      "[LOG 20250307-16:40:02] training status, epoch: [0005/0010], batch: 3000, loss: 0.0029, mode: GPU, time required: 0:00:07.126569\n",
      "[LOG 20250307-16:40:10] training status, epoch: [0005/0010], batch: 4000, loss: 0.0029, mode: GPU, time required: 0:00:07.420538\n",
      "[LOG 20250307-16:40:25] training status, epoch: [0005/0010], loss: 0.0030215250\n",
      "[LOG 20250307-16:40:33] training status, epoch: [0006/0010], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:08.138476\n",
      "[LOG 20250307-16:40:40] training status, epoch: [0006/0010], batch: 2000, loss: 0.0027, mode: GPU, time required: 0:00:07.429438\n",
      "[LOG 20250307-16:40:48] training status, epoch: [0006/0010], batch: 3000, loss: 0.0034, mode: GPU, time required: 0:00:07.376649\n",
      "[LOG 20250307-16:40:55] training status, epoch: [0006/0010], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:07.639095\n",
      "[LOG 20250307-16:41:06] training status, epoch: [0006/0010], loss: 0.0026183648\n",
      "[LOG 20250307-16:41:14] training status, epoch: [0007/0010], batch: 1000, loss: 0.0026, mode: GPU, time required: 0:00:07.308257\n",
      "[LOG 20250307-16:41:21] training status, epoch: [0007/0010], batch: 2000, loss: 0.0026, mode: GPU, time required: 0:00:07.204637\n",
      "[LOG 20250307-16:41:28] training status, epoch: [0007/0010], batch: 3000, loss: 0.0026, mode: GPU, time required: 0:00:07.324148\n",
      "[LOG 20250307-16:41:35] training status, epoch: [0007/0010], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:07.295933\n",
      "[LOG 20250307-16:41:50] training status, epoch: [0007/0010], loss: 0.0026252754\n",
      "[LOG 20250307-16:42:00] training status, epoch: [0008/0010], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:09.589025\n",
      "[LOG 20250307-16:42:07] training status, epoch: [0008/0010], batch: 2000, loss: 0.0026, mode: GPU, time required: 0:00:07.337046\n",
      "[LOG 20250307-16:42:15] training status, epoch: [0008/0010], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:07.216919\n",
      "[LOG 20250307-16:42:22] training status, epoch: [0008/0010], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:07.112506\n",
      "[LOG 20250307-16:42:32] training status, epoch: [0008/0010], loss: 0.0045157108\n",
      "[LOG 20250307-16:42:41] training status, epoch: [0009/0010], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:08.092195\n",
      "[LOG 20250307-16:42:48] training status, epoch: [0009/0010], batch: 2000, loss: 0.0027, mode: GPU, time required: 0:00:07.269323\n",
      "[LOG 20250307-16:42:55] training status, epoch: [0009/0010], batch: 3000, loss: 0.0024, mode: GPU, time required: 0:00:07.341807\n",
      "[LOG 20250307-16:43:02] training status, epoch: [0009/0010], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:07.187080\n",
      "[LOG 20250307-16:43:17] training status, epoch: [0009/0010], loss: 0.0025466550\n",
      "[LOG 20250307-16:43:25] training status, epoch: [0010/0010], batch: 1000, loss: 0.0025, mode: GPU, time required: 0:00:07.798542\n",
      "[LOG 20250307-16:43:33] training status, epoch: [0010/0010], batch: 2000, loss: 0.0026, mode: GPU, time required: 0:00:07.276467\n",
      "[LOG 20250307-16:43:40] training status, epoch: [0010/0010], batch: 3000, loss: 0.0025, mode: GPU, time required: 0:00:07.128016\n",
      "[LOG 20250307-16:43:47] training status, epoch: [0010/0010], batch: 4000, loss: 0.0025, mode: GPU, time required: 0:00:07.106617\n",
      "[LOG 20250307-16:43:57] training status, epoch: [0010/0010], loss: 0.0025416748\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:43:57.438967Z",
     "start_time": "2025-03-07T16:43:57.376489Z"
    }
   },
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = \"ep_10_encoder_model.pth\"\n",
    "decoder_model_name = \"ep_10_decoder_model.pth\"\n",
    "\n",
    "# init training network classes / architectures\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(os.path.join(\"models\", encoder_model_name)))\n",
    "decoder_eval.load_state_dict(torch.load(os.path.join(\"models\", decoder_model_name)))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:44:04.463281Z",
     "start_time": "2025-03-07T16:43:57.453652Z"
    }
   },
   "source": [
    "# convert encoded transactional data to torch Variable\n",
    "data = autograd.Variable(torch_dataset)\n",
    "\n",
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(data))"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:44:08.121350Z",
     "start_time": "2025-03-07T16:44:04.737483Z"
    }
   },
   "source": [
    "# determine reconstruction loss - all transactions\n",
    "reconstruction_loss_all = loss_function(reconstruction, data)\n",
    "\n",
    "# print reconstruction loss - all transactions\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "print('[LOG {}] collected reconstruction loss of: {:06}/{:06} transactions'.format(now, reconstruction.size()[0], reconstruction.size()[0]))\n",
    "print('[LOG {}] reconstruction loss: {:.10f}'.format(now, reconstruction_loss_all.item()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20250307-16:44:08] collected reconstruction loss of: 533009/533009 transactions\n",
      "[LOG 20250307-16:44:08] reconstruction loss: 0.0025416748\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:45:28.595426Z",
     "start_time": "2025-03-07T16:44:58.218685Z"
    }
   },
   "source": [
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
    "\n",
    "    if(i % 100000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
    "pd.Series(reconstruction_loss_transaction).to_csv(\"./results/reconstruction_loss_transaction.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20250307-16:44:58] collected individual reconstruction loss of: 000000/533009 transactions\n",
      "[LOG 20250307-16:45:04] collected individual reconstruction loss of: 100000/533009 transactions\n",
      "[LOG 20250307-16:45:09] collected individual reconstruction loss of: 200000/533009 transactions\n",
      "[LOG 20250307-16:45:15] collected individual reconstruction loss of: 300000/533009 transactions\n",
      "[LOG 20250307-16:45:20] collected individual reconstruction loss of: 400000/533009 transactions\n",
      "[LOG 20250307-16:45:26] collected individual reconstruction loss of: 500000/533009 transactions\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:45:41.989945Z",
     "start_time": "2025-03-07T16:45:41.961663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute the threshold at the 95th percentile\n",
    "quantile_border = 0.99981\n",
    "threshold = np.quantile(reconstruction_loss_transaction, quantile_border)\n",
    "\n",
    "# Flag anomalies: True if the reconstruction loss is above the threshold\n",
    "anomaly_flags = reconstruction_loss_transaction > threshold\n",
    "\n",
    "# Compute the total number of anomalies detected\n",
    "num_anomalies = np.sum(anomaly_flags)\n",
    "\n",
    "print(f\"Anomaly threshold ({quantile_border}th percentile):\", threshold)\n",
    "print(\"Detected anomalies:\", num_anomalies)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold (0.99981th percentile): 0.008751765859423284\n",
      "Detected anomalies: 102\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:45:44.768088Z",
     "start_time": "2025-03-07T16:45:44.659454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(anomaly_flags, pd.Series(label).apply(lambda x: True if x == 'anomal' else False) )"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:45:45.540963Z",
     "start_time": "2025-03-07T16:45:45.530540Z"
    }
   },
   "cell_type": "code",
   "source": "confusion_matrix",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[532906,      1],\n",
       "       [     3,     99]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:44:43.986212500Z",
     "start_time": "2025-03-07T16:31:40.160262Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:44:44.017776500Z",
     "start_time": "2025-03-07T16:31:40.169527Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
